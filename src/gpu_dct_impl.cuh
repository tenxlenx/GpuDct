#pragma once

#include "../include/gpu_dct.cuh"
#include <cmath>
#include <cstdint>
#include <cuda_runtime.h>
#include <device_launch_parameters.h>

namespace gpu_dct {

// ============================================================================
// Constant Memory Declarations
// ============================================================================

__constant__ float c_dct_transform_f32_32[32 * 32];
__constant__ float c_dct_transform_f32_64[64 * 64];
__constant__ double c_dct_transform_f64_32[32 * 32];
__constant__ double c_dct_transform_f64_64[64 * 64];

// ============================================================================
// Type Conversion Kernels
// ============================================================================

/**
 * @brief Convert input type to compute type (e.g., int -> float)
 */
template <typename TIn, typename TOut>
__global__ void kernel_convert_type(const TIn *input, TOut *output, int count) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < count) {
    output[idx] = static_cast<TOut>(input[idx]);
  }
}

// ============================================================================
// DCT Transform Matrix Generation
// ============================================================================

/**
 * @brief Generate DCT transform matrix (Type-II DCT)
 *
 * T[i,j] = c(i) * cos(Ï€ * i * (2j + 1) / (2N))
 * where c(0) = sqrt(1/N), c(i) = sqrt(2/N) for i > 0
 */
template <typename T>
__global__ void kernel_generate_dct_transform(T *output, int n) {
  int i = blockIdx.x;  // Row
  int j = threadIdx.x; // Column

  if (i >= n || j >= n)
    return;

  const T PI = static_cast<T>(3.14159265358979323846);
  const T c_i =
      (i == 0) ? sqrt(static_cast<T>(1) / n) : sqrt(static_cast<T>(2) / n);

  output[i * n + j] = c_i * cos(PI * i * (2 * j + 1) / (2 * n));
}

// ============================================================================
// Fused DCT Kernel - Optimized for 32x32 (Warp-Level)
// ============================================================================

/**
 * @brief Fused DCT kernel optimized for 32x32 matrices
 *
 * Computes: DCT = T * A * T^T in a single kernel pass
 * Uses 32 warps (1024 threads) where each warp computes one row
 * Exploits warp-level parallelism and shared memory
 *
 * Grid: (batch_size, 1, 1)
 * Block: (1024, 1, 1) = 32 warps of 32 threads
 */
template <typename T>
__global__ void kernel_fused_dct_32x32_warp(
    const T *__restrict__ input,     // [batch_size, 32, 32]
    T *__restrict__ output,          // [batch_size, 32, 32]
    const T *__restrict__ transform, // [32, 32] in constant memory
    int batch_size,
    uint64_t *__restrict__ hashes // [batch_size] (optional)
) {
  const int N = 32;
  const int batch_idx = blockIdx.x;

  if (batch_idx >= batch_size)
    return;

  // Each warp computes one row of the output
  const int warp_id = threadIdx.x / 32; // 0-31
  const int lane_id = threadIdx.x % 32; // 0-31

  // Shared memory for input matrix and intermediate result
  __shared__ T s_input[N * N];
  __shared__ T s_temp[N * N];
  __shared__ T s_hash_values[64];
  __shared__ T s_sorted[64];

  // Load input matrix to shared memory (coalesced)
  const T *input_base = input + batch_idx * N * N;
  for (int i = threadIdx.x; i < N * N; i += blockDim.x) {
    s_input[i] = input_base[i];
  }
  __syncthreads();

  // Step 1: Compute T * A -> s_temp
  // Each warp computes one row of T * A
  if (warp_id < N) {
    T sum = 0;
    for (int k = 0; k < N; k++) {
      sum += transform[warp_id * N + k] * s_input[k * N + lane_id];
    }
    s_temp[warp_id * N + lane_id] = sum;
  }
  __syncthreads();

  // Step 2: Compute (T * A) * T^T -> output
  // Each warp computes one row of (T * A) * T^T
  if (warp_id < N) {
    T sum = 0;
    for (int k = 0; k < N; k++) {
      sum += s_temp[warp_id * N + k] * transform[lane_id * N + k];
    }
    if (output) {
      output[batch_idx * N * N + warp_id * N + lane_id] = sum;
    }
    if (hashes && warp_id < 8 && lane_id < 8) {
      s_hash_values[warp_id * 8 + lane_id] = sum;
    }
  }

  __syncthreads();

  if (hashes) {
    const int warp_id = threadIdx.x / 32;
    const int lane_id = threadIdx.x % 32;
    const bool active = (warp_id < 8) && (lane_id < 8);
    const int hash_lane = warp_id * 8 + lane_id;

    if (active) {
      s_sorted[hash_lane] = s_hash_values[hash_lane];
    }
    __syncthreads();

    for (int i = 0; i < 32; ++i) {
      if (active && hash_lane < 63 && (hash_lane % 2 == 0)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();

      if (active && hash_lane < 63 && (hash_lane % 2 == 1)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();
    }

    if (threadIdx.x == 0) {
      const T median = (s_sorted[31] + s_sorted[32]) / static_cast<T>(2);
      uint64_t hash = 0ULL;
      for (int i = 0; i < 64; ++i) {
        if (s_hash_values[i] > median) {
          hash |= (uint64_t{1} << i);
        }
      }
      hashes[batch_idx] = hash;
    }
  }
}

// ============================================================================
// Fused DCT Kernel - Optimized for 64x64
// ============================================================================

/**
 * @brief Fused DCT kernel optimized for 64x64 matrices
 *
 * Uses tiled computation with 32x32 tiles
 * Grid: (batch_size, 2, 2)
 * Block: (32, 32, 1) = 1024 threads
 */
template <typename T>
__global__ void
kernel_fused_dct_64x64(const T *__restrict__ input, T *__restrict__ output,
                       const T *__restrict__ transform, int batch_size,
                       uint64_t *__restrict__ hashes) {
  const int N = 64;
  const int TILE = 32;

  const int batch_idx = blockIdx.x;
  if (batch_idx >= batch_size)
    return;

  const int tile_row = blockIdx.y;
  const int tile_col = blockIdx.z;
  const int tx = threadIdx.x;
  const int ty = threadIdx.y;

  __shared__ T s_A[TILE][TILE + 1]; // +1 to avoid bank conflicts
  __shared__ T s_T[TILE][TILE + 1];
  __shared__ T s_temp[TILE][TILE + 1];
  __shared__ T s_hash_values[64];
  __shared__ T s_sorted[64];

  const T *input_base = input + batch_idx * N * N;
  T *output_base = output + batch_idx * N * N;

  T acc = 0;

  // Compute T * A (tiled)
  for (int tile = 0; tile < (N + TILE - 1) / TILE; tile++) {
    // Load tiles
    int row = tile_row * TILE + ty;
    int col = tile * TILE + tx;
    s_T[ty][tx] = (row < N && col < N) ? transform[row * N + col] : 0;

    row = tile * TILE + ty;
    col = tile_col * TILE + tx;
    s_A[ty][tx] = (row < N && col < N) ? input_base[row * N + col] : 0;

    __syncthreads();

    // Compute partial result
    for (int k = 0; k < TILE; k++) {
      acc += s_T[ty][k] * s_A[k][tx];
    }
    __syncthreads();
  }

  s_temp[ty][tx] = acc;
  __syncthreads();

  // Compute (T * A) * T^T
  acc = 0;
  for (int tile = 0; tile < (N + TILE - 1) / TILE; tile++) {
    // Load transform tile for transpose
    int row = tile * TILE + ty;
    int col = tile_col * TILE + tx;
    s_T[ty][tx] = (row < N && col < N) ? transform[col * N + row] : 0;

    __syncthreads();

    for (int k = 0; k < TILE; k++) {
      acc += s_temp[ty][k] * s_T[tx][k];
    }
    __syncthreads();
  }

  // Write output
  int out_row = tile_row * TILE + ty;
  int out_col = tile_col * TILE + tx;
  if (out_row < N && out_col < N) {
    output_base[out_row * N + out_col] = acc;
    if (hashes && tile_row == 0 && tile_col == 0 && ty < 8 && tx < 8) {
      s_hash_values[ty * 8 + tx] = acc;
    }
  }

  __syncthreads();

  if (hashes && tile_row == 0 && tile_col == 0) {
    const bool active = (ty < 8) && (tx < 8);
    const int hash_lane = ty * 8 + tx;

    if (active) {
      s_sorted[hash_lane] = s_hash_values[hash_lane];
    }
    __syncthreads();

    for (int i = 0; i < 32; ++i) {
      if (active && hash_lane < 63 && (hash_lane % 2 == 0)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();

      if (active && hash_lane < 63 && (hash_lane % 2 == 1)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();
    }

    if (ty == 0 && tx == 0) {
      const T median = (s_sorted[31] + s_sorted[32]) / static_cast<T>(2);
      uint64_t hash = 0ULL;
      for (int i = 0; i < 64; ++i) {
        if (s_hash_values[i] > median) {
          hash |= (uint64_t{1} << i);
        }
      }
      hashes[batch_idx] = hash;
    }
  }
}

// ============================================================================
// General Fused DCT Kernel (128x128, 256x256)
// ============================================================================

/**
 * @brief General fused DCT kernel for larger matrices
 *
 * Uses tiled matrix multiplication
 * Grid: (batch_size, ceil(N/32), ceil(N/32))
 * Block: (32, 32, 1)
 */
template <typename T, int N, int TILE>
__global__ void
kernel_fused_dct_general(const T *__restrict__ input, T *__restrict__ output,
                         const T *__restrict__ transform, int batch_size,
                         uint64_t *__restrict__ hashes) {
  const int batch_idx = blockIdx.x;
  if (batch_idx >= batch_size)
    return;

  const int tile_row = blockIdx.y;
  const int tile_col = blockIdx.z;
  const int tx = threadIdx.x;
  const int ty = threadIdx.y;

  __shared__ T s_A[TILE][TILE + 1];
  __shared__ T s_T[TILE][TILE + 1];
  __shared__ T s_hash_values[64];
  __shared__ T s_sorted[64];

  const T *input_base = input + batch_idx * N * N;
  T *output_base = output + batch_idx * N * N;

  // Step 1: Compute T * A -> temp (stored in global memory)
  T acc1 = 0;

  for (int tile = 0; tile < (N + TILE - 1) / TILE; tile++) {
    // Load tiles
    int row = tile_row * TILE + ty;
    int col = tile * TILE + tx;
    s_T[ty][tx] = (row < N && col < N) ? transform[row * N + col] : 0;

    row = tile * TILE + ty;
    col = tile_col * TILE + tx;
    s_A[ty][tx] = (row < N && col < N) ? input_base[row * N + col] : 0;

    __syncthreads();

    for (int k = 0; k < TILE; k++) {
      acc1 += s_T[ty][k] * s_A[k][tx];
    }
    __syncthreads();
  }

  // Store intermediate result
  int out_row = tile_row * TILE + ty;
  int out_col = tile_col * TILE + tx;
  if (out_row < N && out_col < N) {
    output_base[out_row * N + out_col] = acc1;
  }
  __syncthreads();

  // Step 2: Compute (T * A) * T^T
  T acc2 = 0;

  for (int tile = 0; tile < (N + TILE - 1) / TILE; tile++) {
    // Load tiles
    int row = tile_row * TILE + ty;
    int col = tile * TILE + tx;
    s_A[ty][tx] = (row < N && col < N) ? output_base[row * N + col] : 0;

    row = tile_col * TILE + ty;
    col = tile * TILE + tx;
    s_T[ty][tx] = (row < N && col < N) ? transform[row * N + col] : 0;

    __syncthreads();

    for (int k = 0; k < TILE; k++) {
      acc2 += s_A[ty][k] * s_T[tx][k];
    }
    __syncthreads();
  }

  if (out_row < N && out_col < N) {
    output_base[out_row * N + out_col] = acc2;
    if (hashes && tile_row == 0 && tile_col == 0 && ty < 8 && tx < 8) {
      s_hash_values[ty * 8 + tx] = acc2;
    }
  }

  __syncthreads();

  if (hashes && tile_row == 0 && tile_col == 0) {
    const bool active = (ty < 8) && (tx < 8);
    const int hash_lane = ty * 8 + tx;

    if (active) {
      s_sorted[hash_lane] = s_hash_values[hash_lane];
    }
    __syncthreads();

    for (int i = 0; i < 32; ++i) {
      if (active && hash_lane < 63 && (hash_lane % 2 == 0)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();

      if (active && hash_lane < 63 && (hash_lane % 2 == 1)) {
        if (s_sorted[hash_lane] > s_sorted[hash_lane + 1]) {
          T temp = s_sorted[hash_lane];
          s_sorted[hash_lane] = s_sorted[hash_lane + 1];
          s_sorted[hash_lane + 1] = temp;
        }
      }
      __syncthreads();
    }

    if (ty == 0 && tx == 0) {
      const T median = (s_sorted[31] + s_sorted[32]) / static_cast<T>(2);
      uint64_t hash = 0ULL;
      for (int i = 0; i < 64; ++i) {
        if (s_hash_values[i] > median) {
          hash |= (uint64_t{1} << i);
        }
      }
      hashes[batch_idx] = hash;
    }
  }
}

} // namespace gpu_dct
